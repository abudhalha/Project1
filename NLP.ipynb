{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b407fb0",
   "metadata": {},
   "source": [
    "# Basic Text Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0160891",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34ca9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()[0]+\" \"+ sentence.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17be2067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check whether the word 'qick' belongs to that text\n",
    "\n",
    "'quick' in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d62c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown is present in the sentence\n"
     ]
    }
   ],
   "source": [
    "if \"brown\" in sentence:\n",
    "    print(\"The brown is present in the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8f4341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out the index value of the word 'fox'\n",
    "sentence.index('fox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798b9c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nworb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()[2][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66480a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thedog\n"
     ]
    }
   ],
   "source": [
    "words = sentence.split()\n",
    "first_word = words[0]\n",
    "last_word = words[len(words)-1]\n",
    "concat_word = first_word + last_word\n",
    "print(concat_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e61d3",
   "metadata": {},
   "source": [
    "# Various Steps in NLP\n",
    "\n",
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a827a15f",
   "metadata": {},
   "source": [
    "Tokenization refers to the procedure of spilitting a sentence into its consituent words."
   ]
  },
  {
   "cell_type": "raw",
   "id": "595c7f98",
   "metadata": {},
   "source": [
    "*unigrams\n",
    "*bigrams\n",
    "*trigrams\n",
    "*n-gram refers to sequence of n items from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc8ef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\as\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\as\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\as\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\as\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\as\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\as\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa6e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ce03399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c1b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='Definition. Reinforcement Learning (RL) is the science of decision making It is about learning the optimal behavior in an environment to obtain maximum reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adfd472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5ea570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paragraph']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize('paragraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72641752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize('paragraph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d98a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55b2330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T']\n",
      "['h']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent_tokenize(paragraph))):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a03ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e71b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sky', 'is', 'blue']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('blue', 'JJ')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(\"The sky is blue\")\n",
    "print(words)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8b14a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Definition', '.']\n",
      "[('Definition', 'NN'), ('.', '.')]\n",
      "['Reinforcement', 'Learning', '(', 'RL', ')', 'is', 'the', 'science', 'of', 'decision', 'making', 'It', 'is', 'about', 'learning', 'the', 'optimal', 'behavior', 'in', 'an', 'environment', 'to', 'obtain', 'maximum', 'reward']\n",
      "[('Reinforcement', 'NN'), ('Learning', 'NNP'), ('(', '('), ('RL', 'NNP'), (')', ')'), ('is', 'VBZ'), ('the', 'DT'), ('science', 'NN'), ('of', 'IN'), ('decision', 'NN'), ('making', 'VBG'), ('It', 'PRP'), ('is', 'VBZ'), ('about', 'IN'), ('learning', 'VBG'), ('the', 'DT'), ('optimal', 'JJ'), ('behavior', 'NN'), ('in', 'IN'), ('an', 'DT'), ('environment', 'NN'), ('to', 'TO'), ('obtain', 'VB'), ('maximum', 'JJ'), ('reward', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent_tokenize(paragraph))):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    print(words)\n",
    "    print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48101bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa2ce629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba63e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb19c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am learning Python. It is one of the most popular programming languages\"\n",
    "sentence_words = word_tokenize(sentence)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3852e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages\n"
     ]
    }
   ],
   "source": [
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d68d124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I visited US from UK on 22-10-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14fea204",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentence=sentence.replace(\"US\",\"United States\").replace(\"UK\",\"UnitedKingdom\").replace(\"-18\",\"-2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7af07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United States from UnitedKingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4694903",
   "metadata": {},
   "source": [
    "# NLP Steps\n",
    "\n",
    "\n",
    "*Raw text\n",
    "\n",
    "*Tokenization\n",
    "\n",
    "*Text_cleaning\n",
    "\n",
    "*Pos tagging\n",
    "\n",
    "*Stopwords\n",
    "\n",
    "*Lemmetization ->cleaned text ----> Ml model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaaee67",
   "metadata": {},
   "source": [
    "## Auto correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d30eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\as\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee125669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0724991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'builder'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell=Speller()\n",
    "spell(\"builer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e2a0e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ntural', 'luange', 'processin', 'deals', 'with', 'the', 'art', 'of', 'extracteng']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'lange',\n",
       " 'processing',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'the',\n",
       " 'art',\n",
       " 'of',\n",
       " 'extracting']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = word_tokenize(\"Ntural luange processin deals with the art of extracteng \")\n",
    "print(sentence)\n",
    "from autocorrect import Speller\n",
    "spell = Speller()\n",
    "[spell(word) for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "128f9983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural lange processing deals with the art of extracting\n"
     ]
    }
   ],
   "source": [
    "sentence_corrected = ' '.join([spell(word) for word in sentence])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dffe9",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3523322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer.stem(\"Production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a58bfe36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmer.stem(\"programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c0a67f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairli\n",
      "fire\n",
      "fire\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"Fairly\"))\n",
    "print(stemmer.stem(\"fires\"))\n",
    "print(stemmer.stem(\"firing\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b630ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'footbal'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"football\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b1828",
   "metadata": {},
   "source": [
    "## Lemmatization \n",
    "(it works with dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54a81312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08357b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea0b6526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8df0c",
   "metadata": {},
   "source": [
    "## Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f2295",
   "metadata": {},
   "source": [
    " named entities are usually not present in dictionary so we need to treat them seprately\n",
    "\n",
    "the main objective of the processs to identify the named entities such as proper noun and map them to catogaries that are already defined\n",
    "\n",
    "for ex the catogories might include name of person places and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7523898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\As\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sentence = \"\"\"\"We are reading a book published by packt which is based out of Birmingham.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a326b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  ``/``\n",
      "  We/PRP\n",
      "  are/VBP\n",
      "  reading/VBG\n",
      "  a/DT\n",
      "  book/NN\n",
      "  published/VBN\n",
      "  by/IN\n",
      "  packt/NN\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  based/VBN\n",
      "  out/IN\n",
      "  of/IN\n",
      "  (NE Birmingham/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)),binary=True)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f081fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b153a9",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85442ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "sentence1=\"Keep your savings in the bank\"\n",
    "sentence2=\"It's so risky to drive over the banks of the road\"\n",
    "print(lesk(word_tokenize(sentence1),'bank'))\n",
    "print(lesk(word_tokenize(sentence2),'bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebf083",
   "metadata": {},
   "source": [
    "## Sentence Boundary Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d030c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"\"\"Stemming, in Natural Language Processing (NLP), refers to the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots. While a stemming algorithm is a linguistic normalization process in which the variant forms of a word are reduced to a standard form.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcca4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-z]','',sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# creating the bag of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aaae5c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3f70760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definit': 0,\n",
       " 'reinforcementlearningrlisthescienceofdecisionmakingitisaboutlearningtheoptimalbehaviorinanenvironmenttoobtainmaximumreward': 1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075878df",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "\n",
    "“Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "TF-IDF which means Term Frequency and Inverse Document Frequency, is a scoring measure widely used in information retrieval (IR) or summarization.\n",
    "\n",
    "TF-IDF is intended to reflect how relevant a term is in a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175677dc",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63b07732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cd563a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the  model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce0d643f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.25819889, 0.25819889, 0.        , 0.25819889, 0.51639778,\n",
       "        0.25819889, 0.25819889, 0.25819889, 0.25819889, 0.25819889,\n",
       "        0.25819889, 0.25819889, 0.25819889]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1678a1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.258199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1    2         3         4         5         6         7   \\\n",
       "0  0.000000  0.000000  1.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.258199  0.258199  0.0  0.258199  0.516398  0.258199  0.258199  0.258199   \n",
       "\n",
       "         8         9         10        11        12  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.258199  0.258199  0.258199  0.258199  0.258199  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf39fd",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "\n",
    "SpaCy is a popular and efficient library for natural language processing (NLP) in Python. It provides pre-trained models for various languages, allowing you to perform tasks such as tokenization, part-of-speech tagging, named entity recognition, and more. Here's a brief overview of how to use SpaCy for basic NLP tasks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ebfc18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.7.4 in c:\\users\\as\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (6.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (61.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.21.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (8.2.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (0.3.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (4.64.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (3.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (0.9.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\as\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy==3.7.4) (3.0.4)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\as\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (2.16.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\as\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\as\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\as\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\as\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\as\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\as\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\as\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.4) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\as\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.4) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\as\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.4) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\as\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.7.4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc944096",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"Elon Reeve Musk is a businessman and investor.\n",
    "     He is the founder, chairman, CEO, and CTO of SpaceX, angel investor, CEO, product architect, and former chairman of Tesla,\n",
    "     Inc. owner, executive chairman, and CTO of X Corp. founder of the Boring Company and xAI co-founder of Neuralink and OpenAI, and president of the Musk Foundation. \n",
    "     He is the second wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, \n",
    "     and $182.6 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.\"\"\"\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298e9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
